{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marselj ORG Companies, agencies, institutions, etc.\n",
      "Jaques ORG Companies, agencies, institutions, etc.\n",
      "Konsulatu ORG Companies, agencies, institutions, etc.\n",
      "ÄŒesto ORG Companies, agencies, institutions, etc.\n",
      "Beograd LOC Non-GPE locations, mountain ranges, bodies of water\n",
      "Ministarstvo ORG Companies, agencies, institutions, etc.\n",
      "Beogradu LOC Non-GPE locations, mountain ranges, bodies of water\n",
      "Pozdravite LOC Non-GPE locations, mountain ranges, bodies of water\n",
      "Molim ORG Companies, agencies, institutions, etc.\n",
      "Putniku LOC Non-GPE locations, mountain ranges, bodies of water\n",
      ") ORG Companies, agencies, institutions, etc.\n",
      "Ventimilja ORG Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "import spacy;\n",
    "import json;\n",
    "import tqdm;\n",
    "import sklearn;\n",
    "import text_formatter;\n",
    "\n",
    "from text_formatter import cleaned_text_getter;\n",
    "from spacy.tokens import DocBin;\n",
    "from tqdm import tqdm;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "\n",
    "# Load and clean text data\n",
    "clean_text = cleaned_text_getter();\n",
    "\n",
    "# Load annotated data from JSON file\n",
    "data = json.load(open(\"annotations.json\", 'r', encoding=\"utf8\"));\n",
    "\n",
    "def get_spacy_doc(file, data):\n",
    "    \"\"\"\n",
    "    Converts annotated text data into a spaCy-compatible DocBin object.\n",
    "    \n",
    "    Parameters:\n",
    "    - file: A file object for logging errors.\n",
    "    - data: A list of annotated text samples.\n",
    "\n",
    "    Returns:\n",
    "    - doc_bin: A spaCy DocBin object containing processed documents.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.blank(\"hr\"); # Initialize a blank Croatian NLP model\n",
    "    doc_bin = DocBin(); # Container for serialized spaCy documents\n",
    "\n",
    "    for text, anott in tqdm(data):\n",
    "        doc = nlp.make_doc(text); # Create a spaCy Doc object\n",
    "        anott = anott[\"entities\"]; # Extract entity annotations\n",
    "        ents = [];\n",
    "        entity_indicates = []; # Track entity positions to prevent overlapping\n",
    "\n",
    "        for start, end, label in anott:\n",
    "            skip_entity = False;\n",
    "            \n",
    "            # Check if any part of the entity overlaps with an existing entity\n",
    "            for idx in range(start, end):\n",
    "                if (idx in entity_indicates):\n",
    "                    skip_entity = True;\n",
    "                    break;\n",
    "            \n",
    "            if (skip_entity):\n",
    "                continue;\n",
    "\n",
    "            entity_indicates = entity_indicates + list(range(start, end));\n",
    "\n",
    "            try:\n",
    "                # Create a character span for the entity\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"strict\");\n",
    "            except:\n",
    "                continue; # Skip problematic spans\n",
    "        \n",
    "            if (span is None):\n",
    "                # Log problematic entities for debugging\n",
    "                err_data = str([start, end]) + \"   \" + str(text) + \"\\n\";\n",
    "                file.write(err_data);\n",
    "            else:\n",
    "                ents.append(span);\n",
    "        \n",
    "        try:\n",
    "            doc.ents = ents; # Assign entities to the document\n",
    "            doc_bin.add(doc); # Add the document to the DocBin container\n",
    "        except:\n",
    "            pass; # Skip any errors\n",
    "\n",
    "        return doc_bin;\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%)\n",
    "train, test = train_test_split(data, test_size=0.2);\n",
    "\n",
    "# Open a file to log annotation errors\n",
    "file = open(\"trenirani_modeli/ispis.txt\");\n",
    "\n",
    "# Convert training data to spaCy format and save it\n",
    "doc_bin = get_spacy_doc(file, train);\n",
    "doc_bin.to_disk(\"trenirani_modeli/trenirani_podaci.spacy\");\n",
    "\n",
    "# Convert testing data to spaCy format and save it\n",
    "doc_bin = get_spacy_doc(file, test);\n",
    "doc_bin.to_disk(\"trenirani_modeli/testni_podaci.spacy\");\n",
    "\n",
    "# Close the error log file\n",
    "file.close();\n",
    "\n",
    "# Command to train the spaCy model using the prepared data:\n",
    "#python -m spacy train konfiguracija.cfg  --output trenirani_modeli/output  --paths.train trenirani_modeli/trenirani_podaci.spacy  --paths.dev trenirani_modeli/testni_podaci.spacy\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load(\"trenirani_modeli/output/model-best\");\n",
    "\n",
    "# Process the cleaned text with the trained model\n",
    "doc = nlp(clean_text);\n",
    "\n",
    "# Print recognized entities along with their explanations\n",
    "for e in doc.ents:\n",
    "    print(e.text, e.label_, spacy.explain(e.label_));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
