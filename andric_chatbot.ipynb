{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "# Initialize available models for text generation\n",
    "models = {\n",
    "    \"Meta Llama 3.2 1B\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"Samba Lingo Serbian Chat\": \"sambanovasystems/SambaLingo-Serbian-Chat\",\n",
    "    \"Meta Llama 3.2 1B RS\": \"mradermacher/Llama-3.2-1B-Instruct-RS-GGUF\"\n",
    "};\n",
    "\n",
    "# Function to load the selected model\n",
    "def load_model(model_name):\n",
    "    \"Load the specified model.\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False);\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\");\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=-1)\n",
    "    return generator\n",
    "\n",
    "# Initialize sentence embedding model for retrieval\n",
    "retrieval_model = SentenceTransformer(\"sentence-transformers/LaBSE\");\n",
    "\n",
    "# Knowledge base with letter excerpts from Ivo Andrić\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"category\": \"Knjige i pisma koje je slao Ivo Andric Veri Stojic\",\n",
    "        \"content\": (\"Ivo Andrić je slao knjige i pisma Veri Stojić, ali su se često gubila ili kasnila. \"\n",
    "                    \"Ivo Andrić u svom pismu Veri izražava nezadovoljstvo zbog kašnjenja knjige koju je poslao: '...ne mogu da se načudim da niste primili ni knjigu koju Vam je poslala ovdašnja knjižara Fueri, preporučeno, ni moje pismo koje sam nekoliko dana docnije pisao.' \"\n",
    "                    \"Ivo Andrić je poslao knjigu 'muzički rečnik' Veri jer je smatrao da će joj biti koristan. \"\n",
    "                    \"Ivo Andrić u svom pismu Veri navodi: 'Knjiga koju sam Vam poslao bio je muzički rečnik, mislio sam da će Vam biti koristan i prijatan; urgiraću na pošti.' \"\n",
    "                    \"O čestom kašnjenju pisama između Ive Andrića i Vere Stojić svedoči i sledeća rečenica: '...na moja tri pisma nemam od Vas odgovora, i ako se Vi u svakom vašem pismu žalite da Vam ne pišem.' \"\n",
    "                    \"Vera je na kraju ipak primila i knjigu i pismu čemu svedoči sledeća rečenica iz pisma Ive Andrića: '...primio sam vaše pismo iz kog vidim da ste, ako i sa zadocnjenjem, ipak sve primili u redu, i pisma i knjigu, i da ste živo i zdravo.' \")   \n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Zdravstveno stanje Ive Andrića\",\n",
    "        \"content\": (\"Ivo Andrić se često žali na svoje zdravlje. Najviše pominje grip i anginu. \"\n",
    "                    \"Vezano za svoje zdravlje on kaže: 'Ovde je infernalna vrućina, a ja moram da pijem čajeve i gutam aspirine.' \"\n",
    "                    \"U jednom od svojih pisama Ivo Andrić se žali što je zbog lošeg zdravlja propustio narodno veselje: '...od sinoć ležim, sa anginom i groznicom. Nema ništa od našeg narodnog veselja. Nadam se do sutra, najdalje prekosutra, prezdraviti.' \"\n",
    "                    \"U jednom pismu Ivo Andrić je opisao svoje zdravstveno stanje ovako: 'Kolebam sa zdravljem, a ova proletnja vremena sa vjetrovima i košavama utiču rđavo na živce, naročito kod ljudi koji ni inače nisu posve kako treba.' \"\n",
    "                    \"Ivo Andrić o svom zdravlju navodi i sledeće: '...vrativši se iz Pariza, prebolio (sam) i grip i anginu. Angina je prošla, ali mi je grip još uvek u kostima.'\")\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Boravak Ive Andrića u Mareslju\",\n",
    "        \"content\": (\"Ivo Andrić nije bio zadovoljan svojim boravkom u Marselju. \"\n",
    "                    \"On opisuje život u Marselju kao jednoličan. \"\n",
    "                    \"Nisu mu prijale temperature u Marselju, što se vidi iz dela pisma u kojem piše: 'Ovde infernalna vrućina...'. \"\n",
    "                    \"Nakon povratka u Marselj Ivo Andrić kaže: 'Sad sam opet u Marselju gde život ne pruža mnogo i gde se živi jednolično: između kuće, kancelarije i kavane.' \"\n",
    "                    \"Još jedno njegovo viđenje Marselja on iznosi u pismu: 'Vidim dosta cirkusa i ovde. Prvom prilikom čitaću vam svoje utiske i mišljenja iz Francuske. Oni će biti malo drukčiji nego impresije našeg dragog Duke.'\")\n",
    "    }\n",
    "];\n",
    "\n",
    "\n",
    "# Encode knowledge base texts for similarity search\n",
    "embedded_knowledge = retrieval_model.encode([entry[\"content\"] for entry in knowledge_base]);\n",
    "\n",
    "# Function to find the most relevant text snippet based on the query\n",
    "def retrieve_snippet(query):\n",
    "    \"\"\"Retrieve the most relevant knowledge snippet for a given query.\"\"\"\n",
    "    query_embedded = retrieval_model.encode([query]);                               # Encode the query to obtain its embedding\n",
    "    similarities = retrieval_model.similarity(embedded_knowledge, query_embedded);  # Calculate cosine similarities between the query embedding and the snippet embeddings\n",
    "    best_match_index = similarities.argmax().item();\n",
    "    retrieved_entry = knowledge_base[best_match_index];                             # Retrieve the entry snippet with the highest similarity\n",
    "    return retrieved_entry[\"content\"], retrieved_entry[\"category\"];\n",
    "\n",
    "# Function to generate responses using RAG (Retrieval-Augmented Generation)\n",
    "def ask_with_rag(query, model_choice):\n",
    "    \"\"\"Generate an answer using RAG by retrieving relevant context from the knowledge base.\"\"\"\n",
    "    context, category = retrieve_snippet(query);\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\":\n",
    "                \"You are a helpful AI assistant.\"\n",
    "                \"Provide one Answer ONLY the following query based on the context provided below. \"\n",
    "                \"Provide answer in Serbian language.\"\n",
    "                \"Do not generate or answer any other questions. \"\n",
    "                \"Do not make up or infer any information that is not directly stated in the context. \"\n",
    "                \"Provide a concise answer.\"\n",
    "                f\"Based on the category '{category}', answer the question within the provided context:\\n{context}.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ];\n",
    "    generator = load_model(models[model_choice]);\n",
    "    response = generator(messages, max_new_tokens=128)[-1][\"generated_text\"][-1][\"content\"];\n",
    "    return context, response;\n",
    "\n",
    "# Function to generate responses without RAG (free-form response)\n",
    "def ask_without_rag(query, model_choice):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\" \n",
    "         \"Provide an answer to the user's question.\" \n",
    "         \"Provide answer in Serbian language.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ];\n",
    "\n",
    "    generator = load_model(models[model_choice]);\n",
    "    response = generator(messages, max_new_tokens=128)[-1][\"generated_text\"][-1][\"content\"]\n",
    "    return response;\n",
    "\n",
    "# Function to determine whether to use RAG or not\n",
    "def chat_with_mode(query, use_rag, model_choice):\n",
    "    if use_rag:\n",
    "        context, response = ask_with_rag(query, model_choice);\n",
    "        return f\"Kontekst:\\n{context}\\n Odgovor:\\n{response}\";\n",
    "    else:\n",
    "        response = ask_without_rag(query, model_choice);\n",
    "        return f\"Odgovor:\\n{response}\";\n",
    "\n",
    "# Gradio UI Setup\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1><center>Chatbot</center></h1>\");\n",
    "\n",
    "    model_choice = gr.Dropdown(list(models.keys()), label=\"Izaberite željeni model:\")    \n",
    "    \n",
    "    use_rag = gr.Checkbox(label=\"Koristi RAG\", value=True);\n",
    "\n",
    "    txt_input = gr.Textbox(show_label=False, placeholder=\"Ovdje možeš postaviti pitanje...\");\n",
    "\n",
    "    output = gr.Textbox(label=\"Odgovor\", lines=8);\n",
    "\n",
    "    submit_btn = gr.Button(\"Pitaj\");\n",
    "    submit_btn.click(chat_with_mode, inputs=[txt_input, use_rag, model_choice], outputs=output);\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
